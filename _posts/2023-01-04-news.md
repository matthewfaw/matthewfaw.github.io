---
layout: post
title: News
excerpt_separator: <!--more-->
---


<ul class="date">
  <li class="date">
   <b>[01/12/26]</b> I'll be teaching <a href="https://oscar.gatech.edu/bprod/bwckctlg.p_disp_listcrse?term_in=202602&subj_in=ISYE&crse_in=3770&schd_in=%">ISyE 3770</a>, Statistics and Applications,
   at Georgia Tech this semester.
  </li>
  <li class="date">
   <b>[12/16/25]</b> Our paper on <a href="{{ site.data.links.soda22.arxiv }}">Single-Sample Prophet Inequalities</a> has been accepted to Theory of Computing.
  </li>
  <li class="date">
   <b>[10/27/25]</b> I gave a talk at the INFORMS Job Market Showcase on Online
   Control with Multiple Sensors.
  </li>
  <li class="date">
   <b>[07/02/25]</b> I gave a talk at <a href="https://virtual.oxfordabstracts.com/event/74044/submission/439">INFORMS APS Conference</a> on Bandits with
   Evolving Biased Feedback.
  </li>
  <li class="date">
   <b>[05/01/25]</b> Two papers accepted at ICML'25.
  </li>
  <li class="date">
   <b>[03/03/25]</b> I started as a visiting researcher at Google Research,
   continuing my work with <a href="https://sites.google.com/site/abhidas/">Abhimanyu Das</a>, <a href="https://rajatsen91.github.io/">Rajat Sen</a>, and <a href="https://scholar.google.com/citations?user=E7bEBvAAAAAJ">Yichen Zhou</a>.
  </li>
  <li class="date">
   <b>[10/20/24]</b> I gave a talk on adaptive SGD at <a href="https://submissions.mirasmart.com/InformsAnnual2024/Itinerary/PresentationDetail.aspx?evdid=1716">INFORMS</a>.
  </li>
  <li class="date">
   <b>[10/17/24]</b> I started my postdoc at Georgia Tech.
  </li>
  <li class="date">
   <b>[10/09/24]</b> I successfully defended my Ph.D. thesis.
  </li>
  <li class="date">
   <b>[06/10/24]</b> I started as a Student Researcher at Google Research in
   Mountain View,
   working with <a href="https://sites.google.com/site/abhidas/">Abhimanyu Das</a>, <a href="https://rajatsen91.github.io/">Rajat Sen</a>, and <a href="https://scholar.google.com/citations?user=E7bEBvAAAAAJ">Yichen Zhou</a>.
  </li>
  <li class="date">
   <b>[03/04/24]</b> I gave a talk at Georgia Tech's <a href="https://www.arc.gatech.edu/2023">ARC Colloquium Seminar Series</a> on the Power of Adaptivity in SGD.
  </li>
  <li class="date">
   <b>[12/15/23]</b> I attended the NeurIPS'23 workshop, <a href="https://www.afciworkshop.org/aft2023">Algorithmic Fairness through the Lens of Time</a>, where Jessica and I presented our poster, <a href="{{ site.data.links.neurips23.poster }}">On Mitigating Unconscious Bias through Bandits with Evolving Biased Feedback</a>.
  </li>
  <li class="date">
   <b>[12/07/23]</b> I successfully gave my Progress Review at UT Austin.
  </li>
  <li class="date">
   <b>[09/28/23]</b> I attended <a href="https://allerton.csl.illinois.edu/">Allerton</a>, where Constantine presented our work on <a href="{{ site.data.links.colt23.proceedings }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a> in the Learning and Networks III session.
  </li>
  <li class="date">
   <b>[09/06/23]</b> I am organizing the ML Tea seminar series at UT Austin
   this year. We'll have weekly whiteboard talks by student speakers. Feel free
   to reach out to me if you'd like to give a talk on your work!
  </li>
  <li class="date">
   <b>[07/15/23]</b> I presented our work on <a href="{{ site.data.links.colt23.proceedings }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a> at the Stochastic optimization session at <a href="{{ site.data.links.colt23.session }}">COLT'23</a>. Here are the <a href="{{ site.data.links.colt23.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[06/09/23]</b> I received Dr. Brooks Carlton Fowler Endowed Presidential Graduate Fellowship in Electrical and Computer Engineering from the Cockrell School of Engineering for the 2023-2024 academic year.
  </li>
<!--more-->
  <li class="date">
   <b>[05/14/23]</b> One paper accepted to COLT 2023, <a href="{{ site.data.links.colt23.proceedings }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a>!
  </li>
  <li class="date">
   <b>[04/21/23]</b> I presented our work on <a href="{{ site.data.links.colt23.arxiv }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a> at the <a href="{{ site.data.links.uwworkshop23.link }}">IFML Workshop</a> hosted at University of Washington.
  </li>
  <li class="date">
   <b>[02/13/23]</b> New paper on arXiv, <a href="{{ site.data.links.colt23.arxiv }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a>!
  </li>
  <li class="date">
   <b>[10/06/22]</b> I presented our work on <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD</a> at a poster session in the <a href="{{ site.data.links.simonsworkshop22.link }}">Joint IFML/Data-Driven Decision Processes Workshop</a> at the Simons Institute.
  </li>
  <li class="date">
   <b>[07/26/22]</b> I attended the <a href="{{ site.data.links.santafeworkshop22.link }}">Joint IFML/SFI meeting on Foundations of Machine Learning</a>, where Sanjay presented our work on <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD</a>.
  </li>
  <li class="date">
   <b>[07/04/22]</b> I presented our work on <a href="{{ site.data.links.colt22.proceedings }}">The Power of Adaptivity in SGD</a> in the Optimization I session at <a href="{{ site.data.links.colt22.session }}">COLT'22</a>. Here are the <a href="{{ site.data.links.colt22.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[06/09/22]</b> I presented our work on <a href="{{ sites.data.links.sigmetrics22.proceedings }}">Learning To Maximize Welfare with a Reusable Resource</a> in the Optimization II session at <a href="{{ site.data.links.sigmetrics22.session }}">SIGMETRICS'22</a>. Here are the <a href="{{ site.data.links.sigmetrics22.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[05/14/22]</b> One paper accepted to COLT 2022, <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance</a>!
  </li>
  <li class="date">
   <b>[03/28/22]</b> One paper accepted to SIGMETRICS 2022, <a href="{{ site.data.links.sigmetrics22.proceedings }}">Learning To Maximize Welfare with a Reusable Resource</a>!
  </li>
  <li class="date">
   <b>[02/11/22]</b> New paper on arXiv, <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance</a>!
  </li>
  <li class="date">
   <b>[01/10/22]</b> I presented our work on <a href="{{ site.data.links.soda22.proceedings }}">Single Sample Prophet Inequalities</a> at <a href="{{ site.data.links.soda22.session }}">SODA'22</a>. Here are the <a href="{{ site.data.links.soda22.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[11/02/21]</b> I attended the <a href="{{ site.data.links.simons21.link }}">Joint IFML/CCSI Symposium</a> at Simons in UC Berkeley.
  </li>
  <li class="date">
   <b>[10/02/21]</b> One paper accepted to SODA 2022: <a href="{{ site.data.links.soda22.arxiv }}">Single Sample Prophet Inequalities via Greedy-Ordered Selection</a> (supersedes <a href="{{ site.data.links.soda22.v1 }}">Single-Sample Prophet Inequalities Revisited</a>)
  </li>
  <li class="date">
   <b>[03/24/21]</b> New paper on arXiv, <a href="{{ site.data.links.soda22.v1 }}">Single-Sample Prophet Inequalities Revisited</a>!
  </li>
  <li class="date">
   <b>[12/09/20]</b> I presented our <a href="{{ site.data.links.neurips20.proceedings }}">Mix & Match</a> paper at the virtual NeurIPS 2020 poster session.
  </li>
  <li class="date">
   <b>[09/25/20]</b> One <a href="{{ site.data.links.neurips20.proceedings }}">paper</a> accepted to NeurIPS 2020!
  </li>
  <li class="date">
   <b>[05/26/20]</b> I <a href="{{ site.data.links.nxp.announcement }}">received</a> a fellowship from the <a href="{{ site.data.links.nxp.website }}">NXP Foundation</a> for the 2020-2021 academic year.
  </li>
  <li class="date">
   <b>[11/12/19]</b> I presented a poster of our paper, <a href="{{ site.data.links.neurips20.arxiv }}">Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions</a>, at the Texas Wireless Summit at UT Austin.
  </li>
  <li class="date">
   <b>[10/09/19]</b> New paper on arXiv, <a href="{{ site.data.links.neurips20.arxiv }}">Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions</a>!
  </li>
  <li class="date">
   <b>[08/29/18]</b> I started graduate school at UT Austin!
  </li>
</ul>
