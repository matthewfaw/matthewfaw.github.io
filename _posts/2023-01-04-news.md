---
layout: post
title: News
excerpt_separator: <!--more-->
---


<ul class="date">
  <li class="date">
   <b>[09/28/23]</b> I attended <a href="https://allerton.csl.illinois.edu/">Allerton</a>, where Constantine presented our work on <a href="{{ site.data.links.colt23.proceedings }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a> in the Learning and Networks III session.
  </li>
  <li class="date">
   <b>[09/06/23]</b> I am organizing the ML Tea seminar series at UT Austin
   this year. We'll have weekly whiteboard talks by student speakers. Feel free
   to reach out to me if you'd like to give a talk on your work!
  </li>
  <li class="date">
   <b>[07/15/23]</b> I presented our work on <a href="{{ site.data.links.colt23.proceedings }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a> at the Stochastic optimization session at <a href="{{ site.data.links.colt23.session }}">COLT'23</a>. Here are the <a href="{{ site.data.links.colt23.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[06/09/23]</b> I received Dr. Brooks Carlton Fowler Endowed Presidential Graduate Fellowship in Electrical and Computer Engineering from the Cockrell School of Engineering for the 2023-2024 academic year.
  </li>
  <li class="date">
   <b>[05/14/23]</b> One paper accepted to COLT 2023, <a href="{{ site.data.links.colt23.proceedings }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a>!
  </li>
  <li class="date">
   <b>[04/21/23]</b> I presented our work on <a href="{{ site.data.links.colt23.arxiv }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a> at the <a href="{{ site.data.links.uwworkshop23.link }}">IFML Workshop</a> hosted at University of Washington.
  </li>
  <li class="date">
   <b>[02/13/23]</b> New paper on arXiv, <a href="{{ site.data.links.colt23.arxiv }}">Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD</a>!
  </li>
  <li class="date">
   <b>[10/06/22]</b> I presented our work on <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD</a> at a poster session in the <a href="{{ site.data.links.simonsworkshop22.link }}">Joint IFML/Data-Driven Decision Processes Workshop</a> at the Simons Institute.
  </li>
  <li class="date">
   <b>[07/26/22]</b> I attended the <a href="{{ site.data.links.santafeworkshop22.link }}">Joint IFML/SFI meeting on Foundations of Machine Learning</a>, where Sanjay presented our work on <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD</a>.
  </li>
<!--more-->
  <li class="date">
   <b>[07/04/22]</b> I presented our work on <a href="{{ site.data.links.colt22.proceedings }}">The Power of Adaptivity in SGD</a> in the Optimization I session at <a href="{{ site.data.links.colt22.session }}">COLT'22</a>. Here are the <a href="{{ site.data.links.colt22.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[06/09/22]</b> I presented our work on <a href="{{ sites.data.links.sigmetrics22.proceedings }}">Learning To Maximize Welfare with a Reusable Resource</a> in the Optimization II session at <a href="{{ site.data.links.sigmetrics22.session }}">SIGMETRICS'22</a>. Here are the <a href="{{ site.data.links.sigmetrics22.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[05/14/22]</b> One paper accepted to COLT 2022, <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance</a>!
  </li>
  <li class="date">
   <b>[03/28/22]</b> One paper accepted to SIGMETRICS 2022, <a href="{{ site.data.links.sigmetrics22.proceedings }}">Learning To Maximize Welfare with a Reusable Resource</a>!
  </li>
  <li class="date">
   <b>[02/11/22]</b> New paper on arXiv, <a href="{{ site.data.links.colt22.arxiv }}">The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance</a>!
  </li>
  <li class="date">
   <b>[01/10/22]</b> I presented our work on <a href="{{ site.data.links.soda22.proceedings }}">Single Sample Prophet Inequalities</a> at <a href="{{ site.data.links.soda22.session }}">SODA'22</a>. Here are the <a href="{{ site.data.links.soda22.slides | relative_url }}">slides</a>.
  </li>
  <li class="date">
   <b>[11/02/21]</b> I attended the <a href="{{ site.data.links.simons21.link }}">Joint IFML/CCSI Symposium</a> at Simons in UC Berkeley.
  </li>
  <li class="date">
   <b>[10/02/21]</b> One paper accepted to SODA 2022: <a href="{{ site.data.links.soda22.arxiv }}">Single Sample Prophet Inequalities via Greedy-Ordered Selection</a> (supersedes <a href="{{ site.data.links.soda22.v1 }}">Single-Sample Prophet Inequalities Revisited</a>)
  </li>
  <li class="date">
   <b>[03/24/21]</b> New paper on arXiv, <a href="{{ site.data.links.soda22.v1 }}">Single-Sample Prophet Inequalities Revisited</a>!
  </li>
  <li class="date">
   <b>[12/09/20]</b> I presented our <a href="{{ site.data.links.neurips20.proceedings }}">Mix & Match</a> paper at the virtual NeurIPS 2020 poster session.
  </li>
  <li class="date">
   <b>[09/25/20]</b> One <a href="{{ site.data.links.neurips20.proceedings }}">paper</a> accepted to NeurIPS 2020!
  </li>
  <li class="date">
   <b>[05/26/20]</b> I <a href="{{ site.data.links.nxp.announcement }}">received</a> a fellowship from the <a href="{{ site.data.links.nxp.website }}">NXP Foundation</a> for the 2020-2021 academic year.
  </li>
  <li class="date">
   <b>[10/09/19]</b> New paper on arXiv, <a href="{{ site.data.links.neurips20.arxiv }}">Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions</a>!
  </li>
  <li class="date">
   <b>[08/29/18]</b> I started graduate school at UT Austin!
  </li>
</ul>
